{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the comments of RNN code example in keras|recurrent neural network sentiment analysis\n",
    "#bcz..some changes are done like 17-->18  ,,10000 --> 88586,,,in unique words in docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vR6C9i-fyLvG"
   },
   "outputs": [],
   "source": [
    "docs = ['go india',\n",
    "\t\t'india india',\n",
    "\t\t'hip hip hurray',\n",
    "\t\t'jeetega bhai jeetega india jeetega',\n",
    "\t\t'bharat mata ki jai',\n",
    "\t\t'kohli kohli',\n",
    "\t\t'sachin sachin',\n",
    "\t\t'dhoni dhoni',\n",
    "\t\t'modi ji ki jai',\n",
    "\t\t'inquilab zindabad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE->u first do integer encoding(padding also),,and after that only give the vectors(sparse vectors ,bcz after doing padding more zero values are there in each vector,,bcz to make all vectors of same length) to embeddings layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xRzvJNh1mRQW"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2IhkPg6YmRYT"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dP5g_Tmin4AG",
    "outputId": "219ad8f6-9390-4da8-afcd-d51a13a5b1e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)\n",
    "#as u see 17 unique words(vocabs) in our dataset or document-->docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOv8NrFFmRcE",
    "outputId": "eb3fbafe-991a-450a-e4f8-2988c5df6525"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 1],\n",
       " [1, 1],\n",
       " [3, 3, 10],\n",
       " [2, 11, 2, 1, 2],\n",
       " [12, 13, 4, 5],\n",
       " [6, 6],\n",
       " [7, 7],\n",
       " [8, 8],\n",
       " [14, 15, 4, 5],\n",
       " [16, 17]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "sequences\n",
    "#10 vectors are there,,,with diff length,,and largest length of vector is 5 ,,so padding is done such that all vectors become of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KG1RqcLnmRkA",
    "outputId": "d497ded2-754f-457b-80f3-a4a69bbf1373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  1,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  0],\n",
       "       [ 3,  3, 10,  0,  0],\n",
       "       [ 2, 11,  2,  1,  2],\n",
       "       [12, 13,  4,  5,  0],\n",
       "       [ 6,  6,  0,  0,  0],\n",
       "       [ 7,  7,  0,  0,  0],\n",
       "       [ 8,  8,  0,  0,  0],\n",
       "       [14, 15,  4,  5,  0],\n",
       "       [16, 17,  0,  0,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "sequences = pad_sequences(sequences,padding='post')\n",
    "sequences\n",
    "#as u see maximum values are zero in most of the vector,,so these vectors are known as sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETWEOn4wmRzd",
    "outputId": "b0ac8b69-117d-466c-ae34-422ffe7a90e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 5, 2)              36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36 (144.00 Byte)\n",
      "Trainable params: 36 (144.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(18,output_dim=2,input_length=5))#embedding layer\n",
    "#17 unique words are there in docs(our dataset),,but we have to take 17+1,,0/w error,,output_dim=2->dimension of each dense vector(this is hyperparameter ,,u can give any value which gives best accuracy,at the time of hyperparameter tuning u can try this)\n",
    "#input_length=5->length of each vector(sparse vector),,which is given as i/p to embedding layer\n",
    "#see notes,,for clearity\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "R68ghDfNmSAC"
   },
   "outputs": [],
   "source": [
    "model.compile('adam','accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooYWO2NwoKoQ",
    "outputId": "62b763d3-07b0-49d5-a2ca-23fe55c4ee7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step\n",
      "[[[ 0.02960292 -0.00890279]\n",
      "  [-0.01030905  0.04993207]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[-0.01030905  0.04993207]\n",
      "  [-0.01030905  0.04993207]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[ 0.01696435 -0.00319557]\n",
      "  [ 0.01696435 -0.00319557]\n",
      "  [ 0.01565382  0.04619518]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[ 0.00580437 -0.0165373 ]\n",
      "  [ 0.02883548  0.02850988]\n",
      "  [ 0.00580437 -0.0165373 ]\n",
      "  [-0.01030905  0.04993207]\n",
      "  [ 0.00580437 -0.0165373 ]]\n",
      "\n",
      " [[ 0.01948109 -0.03611461]\n",
      "  [-0.00607592  0.00232766]\n",
      "  [-0.03265493  0.01316612]\n",
      "  [-0.00671235  0.02017167]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[ 0.02364283  0.01796317]\n",
      "  [ 0.02364283  0.01796317]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[-0.04604709 -0.00656825]\n",
      "  [-0.04604709 -0.00656825]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[-0.01527923  0.01435376]\n",
      "  [-0.01527923  0.01435376]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[ 0.01344199  0.02292245]\n",
      "  [ 0.04620228 -0.02839513]\n",
      "  [-0.03265493  0.01316612]\n",
      "  [-0.00671235  0.02017167]\n",
      "  [-0.03302218  0.01160396]]\n",
      "\n",
      " [[ 0.01056128 -0.00537741]\n",
      "  [ 0.04029038 -0.02023245]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]\n",
      "  [-0.03302218  0.01160396]]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(sequences)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yrM0IXVamPLn"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Embedding,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4aH4HVjcyq1f"
   },
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cu2UOZGUzAEF"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train,padding='post',maxlen=50)#shortning the review,,,bcz we are demonstarting how to use embedding,so to reduce avoid trining time we shorten the length of each review ie;length of each review is 50(ie;first 50 words of each review is slected,,note->if review contains less than 50 words then zeros is added)padding to make it of length 50)\n",
    "#if u want to train all data,and u have time ,,then u can remove this maxlen,,thus train complete review,,thus u can get more accuracy aswell\n",
    "X_test = pad_sequences(X_test,padding='post',maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO412Krkz7EO",
    "outputId": "b4f2de4f-c1ba-4d76-c8b9-cf7087850a3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWXhm8vP_DO-",
    "outputId": "c3eb5f20-a438-4439-b5fe-640f215fb61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 50, 2)             177172    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                1120      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178325 (696.58 KB)\n",
      "Trainable params: 178325 (696.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(88586,output_dim=2,input_length=50))#unique words/vocablary size is 88585 but we take 88585+1_simple_rnn_architecture.ipynb`\n",
    "model.add(SimpleRNN(32,return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "#vvvimp\n",
    "#88586*32=>177,172.....no biases for embedding layer\n",
    "#2*32=>64,,,,32*32=>1024,,,,64+1024+32=>1120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGCAZ7Rm_fqH",
    "outputId": "6f6a73ab-fbd4-4a23-ea56-46e3f9358c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 18s 21ms/step - loss: 0.6232 - acc: 0.6106 - val_loss: 0.4461 - val_acc: 0.7950\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.3758 - acc: 0.8373 - val_loss: 0.4410 - val_acc: 0.8081\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.2546 - acc: 0.9016 - val_loss: 0.4708 - val_acc: 0.8048\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.1814 - acc: 0.9338 - val_loss: 0.6246 - val_acc: 0.7844\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.1351 - acc: 0.9536 - val_loss: 0.6016 - val_acc: 0.7885\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,epochs=5,validation_data=(X_test,y_test))\n",
    "#by defaulty use mini batch used and batch size is selcted automatically by keras by seeing dataset....so 25k/32=>782 times w,b are updated per epoch,,,,ie;by seeing 32 rows 782 times w,b are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAQETwiZikEY"
   },
   "outputs": [],
   "source": [
    "#as u see even if we run rnn on shorten reviewsof imdb(ie;50 words from each review is selected then also ,,accuracy is high),,so imagine if u run it by taking complete wrods of each review then accuracy is high,,,,and apply some hyperparametr tuning and other methods to increase accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
