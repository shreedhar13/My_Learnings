{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f0e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense,Conv2D,Flatten\n",
    "from keras import Sequential\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9223c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "#It is GreyScale image...so image is represented in 2d and thus filter/kernel also 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc18aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(28,28,1)))#C_L1(layer1)\n",
    "#input_shape=(28,28,1),,,NOTE_> in ann(fully connected NN) we use input_dim,,in cnn we use input_shape \n",
    "#32 filters in convolutional layer1(C_L1),,and each with size 3x3,,padding='valid' means  padding is 'NOT' applied\n",
    "#activation='relu'  ?  \n",
    "#suppose C_L using \"vertical edge detecting filter\"(it can be anything,i just take one eg) ,then C_L1 produce \"feature map\"\n",
    "#and represent  all vertical edge present in passed i/p image by pixel info,,,if model is sure that these edges/lines\n",
    "#are 100% vertical among all captured one then it give +ve pixel value(black/white if greayscale image OR color code of that edge if RGB image)\n",
    "#and remaining edges,on which model is not sure,(ie;it may be vertical or not vertical also) for that pixel value is given as -ve\n",
    "#therefore when feature map passed to activation then relu is used bcz it make -ve value as 0 and leave +ve value as it is\n",
    "#thus now feature map have 100% confirmed vertical line's pixel values,,and remaining with 0,,,so relu used most\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu'))#C_L2(layer2),,,3x3x32 filters representation..ie;32 filters each with 3 rows x 3 cols\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu'))#C_L3(layer3)\n",
    "#Convolutional layers are ended\n",
    "\n",
    "model.add(Flatten())#converting received data from C_L3 into 1d and passed to fully connected NN\n",
    "#is flatten is a separate layer?-->Usually no. Flatten is usually considered an operation, like an activation function, that is applied to an output. They provide “flatten” as a layer so that it’s easier to use; you don’t have to use some backend function or deal with the gradient calculation on your own.\n",
    "#The best way I can think to put it is if you are counting the number of layers, then flatten should not be counted. But if you’re drawing a diagram of what is in your neural network, you should include it because it’s an operation and will help people understand your network’s architecture.\n",
    "\n",
    "#fully connected layer starts\n",
    "model.add(Dense(128,activation='relu'))#(layer4),,,takes i/p from flatten,,so no need to mention input_dim,,internally identified\n",
    "model.add(Dense(10,activation='softmax'))#(layer5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11458ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 22, 32)        9248      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 15488)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1982592   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2002698 (7.64 MB)\n",
      "Trainable params: 2002698 (7.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#  conv2d_1 (Conv2D)           (None, 26, 26, 32)  ,32 feature maps are there and each with 26rowsx26cols (but in normal array 2x3x4) means 3rows and 4 cols like this 2 arrays are present,,,,and 28x28 is 1 image size\n",
    "#(28-3+1)X(28-3+1)=>26X26 size of 1 feature map(26X26X1) ,,,and such a 32 feature space are there bcz 32 filters are there so,,26X26X32 is feature map volume..3*3=9-->9*32=>288-->288(weights)+32(bias)-->320\n",
    "#this(26x26x32) is i/p to conv2d_2 \n",
    "\n",
    "#  conv2d_2 (Conv2D)           (None, 24, 24, 32)    ..(26-3+1)x(26-3+1)X32\n",
    "#this(24x24x32) is i/p to conv2d_3                                                                  \n",
    "\n",
    "#  conv2d_3 (Conv2D)           (None, 22, 22, 32)  \n",
    "\n",
    "#as u see feature map shape is decresing layer by layer bcz padding='valid' thus 2 problem occurse(discussed in book),,so use \"padding\"..\n",
    "#flatten layers o/p is 22*22*32=>15488...imagine bro,,ur smart enough\n",
    "\n",
    "#15488*128(weights)+128(bias)==>1982592\n",
    "#128*10+10=>1290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca65c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding is done.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',activation='relu',input_shape=(28,28,1)))#C_L1(layer1)\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',activation='relu'))#C_L2(layer2)\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',activation='relu'))#C_L3(layer3)\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#fully connected layer starts\n",
    "model.add(Dense(128,activation='relu'))#(layer4),\n",
    "model.add(Dense(10,activation='softmax'))#(layer5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d010032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               3211392   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3231498 (12.33 MB)\n",
      "Trainable params: 3231498 (12.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#as u see each feature map is of size same as i image size ie;28X28,,,therefore no \"info loss\" and all pixel parcipate in convolution operation in more than 2 times\n",
    "#as u see weights and biases are not dependent on image size,,,,,,only o/p size of flatten is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f708c1",
   "metadata": {},
   "source": [
    "# strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c48e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',strides=(2,2), activation='relu', input_shape=(28,28,1)))#strides=(2,2),,u an give like this also,,strides=(2,1) or (1,2)  anything,,,and visualize  feature map size,by yourself by modyfing formula from book\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',strides=(2,2), activation='relu'))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='same',strides=(2,2), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c4a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 7, 7, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 32)          9248      \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85770 (335.04 KB)\n",
      "Trainable params: 85770 (335.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#information lost\n",
    "#only big-big thing which are noticable by human also ,,is captured,,and very minute/small small things from data is ignore,,thus info lost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
