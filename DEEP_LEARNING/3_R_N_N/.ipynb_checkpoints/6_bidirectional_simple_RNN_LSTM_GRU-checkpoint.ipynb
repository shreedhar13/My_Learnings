{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a13e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense,LSTM,GRU,Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7739094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 5)                 190       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320196 (1.22 MB)\n",
      "Trainable params: 320196 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "x_train = pad_sequences(x_train, maxlen=100,padding='post',truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=100,padding='post',truncating='post')\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100), # Embedding layer to convert words to vectors....i/p has 10k unique words..each vector is of 32 dimension ie;every word represented with 32 dimension vector\n",
    "    SimpleRNN(5),   # RNN layer with 5 units,,u can take any no of nodes\n",
    "    Dense(1, activation='sigmoid')          # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da0d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a881605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.6962 - accuracy: 0.5092 - val_loss: 0.6905 - val_accuracy: 0.5302\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.6124 - accuracy: 0.6823 - val_loss: 0.7313 - val_accuracy: 0.5444\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.4543 - accuracy: 0.7996 - val_loss: 0.8412 - val_accuracy: 0.5406\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.3313 - accuracy: 0.8669 - val_loss: 0.9813 - val_accuracy: 0.5324\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.2544 - accuracy: 0.9046 - val_loss: 1.1087 - val_accuracy: 0.5338\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9956c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 10)                380       \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320391 (1.22 MB)\n",
      "Trainable params: 320391 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bidirectional simple rnn\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100), # Embedding layer to convert words to vectors....i/p has 10k unique words..each vector is of 32 dimension ie;every word represented with 32 dimension vector\n",
    "    Bidirectional(SimpleRNN(5)),   # bidirectional RNN layer with 5 units,,u can take any no of nodes\n",
    "    #we used Bidirectional wrapper\n",
    "    Dense(1, activation='sigmoid')          # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea26b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#190 x 2 ==>380...bidirectinal rnn' single layer(h_L or recurrent layer) is there ..\n",
    "#u can add multiple bidirectional rnn to form deep bidirectional rnn\n",
    "#but mostly LSTM used for simple/deep/bidirectional functionality,,,bcz best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aebde17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a49979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.5216 - accuracy: 0.7528 - val_loss: 0.4912 - val_accuracy: 0.7794\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.3783 - accuracy: 0.8407 - val_loss: 0.4927 - val_accuracy: 0.7842\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 0.2822 - accuracy: 0.8875 - val_loss: 0.4951 - val_accuracy: 0.7818\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.1894 - accuracy: 0.9324 - val_loss: 0.5743 - val_accuracy: 0.7810\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.1378 - accuracy: 0.9519 - val_loss: 0.5930 - val_accuracy: 0.7842\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5deb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 10)                1520      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321531 (1.23 MB)\n",
      "Trainable params: 321531 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#biLSTM(bidirectional lstm)\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100), # Embedding layer to convert words to vectors....i/p has 10k unique words..each vector is of 32 dimension ie;every word represented with 32 dimension vector\n",
    "    Bidirectional(LSTM(5)),   # RNN layer with 5 units,,u can take any no of nodes\n",
    "    Dense(1, activation='sigmoid')          # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858b35d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 10)                1170      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321181 (1.23 MB)\n",
      "Trainable params: 321181 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#biGRU(bidirectional GRU)\n",
    "model = Sequential([\n",
    "    Embedding(10000, 32, input_length=100), # Embedding layer to convert words to vectors....i/p has 10k unique words..each vector is of 32 dimension ie;every word represented with 32 dimension vector\n",
    "    Bidirectional(GRU(5)),   # RNN layer with 5 units,,u can take any no of nodes\n",
    "    Dense(1, activation='sigmoid')          # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6544e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
